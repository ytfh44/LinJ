"""
Product Name Generator Demo

Demonstrates how to create simple workflows using the LinJ framework:
1. Extract keywords (ToolNode)
2. Generate prompts (HintNode)
3. Call LLM to generate names (ToolNode)
4. Validate and output (JoinNode)
5. Quality gate (GateNode)

Supports two backends:
- AutoGen: uses linj_autogen.executor.runner
- LangGraph: uses langgraph module
"""

import asyncio
import sys
import os
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable

# Add project root to Python path
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.insert(0, str(project_root))

# Ensure src directory is in path
src_path = project_root / "autogen" / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))


def get_backend(backend_type: str = "autogen"):
    """
    Get backend executor of specified type

    Args:
        backend_type: "autogen" or "langgraph"

    Returns:
        Backend object
    """
    if backend_type.lower() == "langgraph":
        # Try importing LangGraph backend
        try:
            from examples.langgraph_backend import LangGraphBackend

            return LangGraphBackend(enable_tracing=True)
        except ImportError:
            print("Warning: LangGraph backend not available, falling back to AutoGen")
            return get_backend("autogen")
    else:
        # Default use AutoGen backend
        from linj_autogen.executor.runner import LinJExecutor

        return LinJExecutor(enable_tracing=True)


# --- Tool Functions ---


async def extract_keywords(description: str) -> List[str]:
    """
    Extract keywords from product description

    Args:
        description: Product description text

    Returns:
        List of extracted keywords
    """
    print(f"[Tool: ExtractKeywords] Input: {description}")

    # Simple logic: extract words with length > 4
    words = description.split()
    keywords = [w for w in words if len(w) > 4]

    print(f"[Tool: ExtractKeywords] Output: {keywords}")
    return keywords


async def ollama_llm(prompt: str, model: str = "qwen3:0.6b") -> str:
    """
    Call LLM via Ollama to generate content

    Args:
        prompt: Prompt text
        model: Model name

    Returns:
        Content generated by LLM
    """
    print(f"[Tool: Ollama] Sending prompt to {model}...")

    try:
        import httpx

        url = "http://localhost:11434/api/generate"
        payload = {"model": model, "prompt": prompt, "stream": False}

        async with httpx.AsyncClient(timeout=60.0, trust_env=False) as client:
            response = await client.post(url, json=payload)
            response.raise_for_status()
            result = response.json().get("response", "")

            print(f"[Tool: Ollama] Received response ({len(result)} chars)")
            return result.strip()

    except Exception as e:
        print(f"[Tool: Ollama] Error: {e}")
        # Return mock result as fallback
        mock_response = (
            "1. SuperProduct\n"
            "2. MegaTool\n"
            "3. UltraService\n"
            "4. PowerHouse\n"
            "5. PrimeChoice"
        )
        return mock_response


# --- Run Scenarios ---


async def run_scenario(
    name: str, input_desc: str, expect_fail: bool = False, backend_type: str = "autogen"
):
    """
    Run specified scenario

    Args:
        name: Scenario name
        input_desc: Input description
        expect_fail: Whether failure is expected
        backend_type: Backend type ("autogen" or "langgraph")
    """
    print(f"\n{'=' * 60}")
    print(f"--- Running Scenario: {name} (Backend: {backend_type}) ---")
    print(f"{'=' * 60}")

    # 1. Load document
    yaml_path = current_dir / "workflow.yaml"

    # Select import based on backend type
    if backend_type.lower() == "langgraph":
        try:
            from examples.langgraph_backend import load_document
        except ImportError:
            from linj_autogen.executor.runner import load_document
    else:
        from linj_autogen.executor.runner import load_document

    doc = load_document(str(yaml_path))

    # 2. Initialize executor
    executor = get_backend(backend_type)

    # 3. Register tools
    executor.register_tool("extract_keywords", extract_keywords)
    executor.register_tool("mock_llm", ollama_llm)

    # 4. Initial state
    initial_state = {"input": {"description": input_desc}}

    try:
        # 5. Execute workflow
        final_state = await executor.run(doc, initial_state)

        print("\n>> Execution Completed Successfully!")
        print(f">> Final Names:\n{final_state.get('final_names')}")

        if expect_fail:
            print(">> Note: Scenario completed without validation error")

    except Exception as e:
        if expect_fail:
            print(f">> Caught EXPECTED error: {type(e).__name__}: {e}")
        else:
            print(f"!! UNEXPECTED Execution Failed: {type(e).__name__}: {e}")
            raise


async def main(backend_type: str = "autogen"):
    """
    Main function

    Args:
        backend_type: Backend type to use
    """
    print(f"=== Product Name Generator Demo ===")
    print(f"Using backend: {backend_type}")
    print(f"Project root: {project_root}")

    # Scenario 1: Valid input
    await run_scenario(
        "Valid Product",
        "This is a fantastic machine that makes coffee.",
        expect_fail=False,
        backend_type=backend_type,
    )

    # Scenario 2: Invalid input (triggers glossary check)
    await run_scenario(
        "Invalid Product (triggers glossary check)",
        "This is a investment scheme with high risk.",
        expect_fail=True,
        backend_type=backend_type,
    )


def parse_args():
    """Parse command line arguments"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Product Name Generator Demo",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py --backend autogen      # Use AutoGen backend
  python main.py --backend langgraph    # Use LangGraph backend
  python main.py --backend all          # Run both backends sequentially
        """,
    )

    parser.add_argument(
        "--backend",
        choices=["autogen", "langgraph", "all"],
        default="autogen",
        help="Select backend type (default: autogen)",
    )

    parser.add_argument(
        "--scenario",
        choices=["valid", "invalid", "all"],
        default="all",
        help="Select scenario to run (default: all)",
    )

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    if args.backend == "all":
        # Run both backends sequentially
        for backend in ["autogen", "langgraph"]:
            print(f"\n\n{'#' * 60}")
            print(f"# Running with {backend.upper()} backend")
            print(f"{'#' * 60}")
            asyncio.run(main(backend_type=backend))
    else:
        # Run specified backend
        asyncio.run(main(backend_type=args.backend))
